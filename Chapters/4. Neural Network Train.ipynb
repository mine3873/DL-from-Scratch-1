{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d031208e",
   "metadata": {},
   "source": [
    "# 신경망 학습\n",
    "> **학습**이란, 훈련 데이터로부터 신경망 내의 가중치 매개변수의 최적값을 찾아내는 과정을 의미한다.  \n",
    "> 이때, **손실 함수**를 최소화함으로써 최적값을 찾는다.\n",
    "\n",
    "## Loss function\n",
    "### Mean Squared Error, MSE\n",
    "평균 제곱 오차 MSE 는 다음과 같다.\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{RSS}(w) &\\triangleq \\sum_{n=1}^{N} (y_{n} - w^{T} x_{n})^{2} \\\\\n",
    "\\text{MSE}(w) &\\triangleq \\frac{1}{N}\\text{RSS}(w)\n",
    "\\end{align}\n",
    "$$\n",
    "즉, 입력-출력 쌍 $(x_n,y_n)$에 대해서, 실제 출력값과 모델 예측값의 차이를 제곱 평균 한 것과 같다.  \n",
    "\n",
    "#### CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f29b04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def mean_squared_error(y, x):\n",
    "    return np.mean((y - x) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e771404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.019500000000000007\n"
     ]
    }
   ],
   "source": [
    "x = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])\n",
    "y = np.array([0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0])\n",
    "print(mean_squared_error(y, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371d2b4a",
   "metadata": {},
   "source": [
    "### Cross Entropy Error, CEE\n",
    "교차 엔트로피 오차는 다음과 같다.\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{CEE}(w) &\\triangleq -\\sum_{k=1}^{K} y_{k} \\log \\hat{y}_{k}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "이때 $y_k$는 실제 참값, $\\hat{y}_k$는 모델이 예측한 값을 의미한다.\n",
    "\n",
    "#### CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd3740f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y_pred, y_true):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(y_true * np.log(y_pred + delta)) # add delta to avoid log(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefddc7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.510825457099338\n"
     ]
    }
   ],
   "source": [
    "y_true = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])\n",
    "y_pred = np.array([0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0])\n",
    "print(cross_entropy_error(y_pred, y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce91eeed",
   "metadata": {},
   "source": [
    "### 미니배치 학습\n",
    "이전까지는 데이터 한개에 대한 손실 함수를 보았으므로, 다수의 데이터에 대한 손실 함수를 구현해보자. 위의 교차 엔트로피 오차의 식을 확장시키면, 다음과 같다.\n",
    "$$\n",
    "\\text{CEE}(w) \\triangleq - \\frac{1}{N} \\sum_{n=1}^{N}\\sum_{k=1}^{K} y_{nk}\\log(\\hat{y}_{nk})\n",
    "$$\n",
    "이떄 $y_{nk}$는 $n$번째 데이터의 $k$번째 참값이고, $\\hat{y}_{nk}$은 같은 부분의 모델이 예측한 값을 의미한다.  \n",
    "**미니배치 학습**은 주어진 데이터 전체를 사용하는 것이 아닌, 데이터의 일부만을 사용하여 학습을 진행하는 방법을 말한다.  \n",
    "이전에 살펴본 MNIST 데이터셋을 사용하여, 미니배치 학습을 구현해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7195b9fa",
   "metadata": {},
   "source": [
    "#### CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34b5f60",
   "metadata": {},
   "source": [
    "##### 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4651330a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape = (60000, 28, 28)\n",
      "y_train.shape = (60000,)\n",
      "x_test.shape = (10000, 28, 28)\n",
      "y_test.shape = (10000,)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "print(\"x_train.shape =\", x_train.shape)  # (60000, 28, 28)\n",
    "print(\"y_train.shape =\", y_train.shape)  # (60000,)\n",
    "print(\"x_test.shape =\", x_test.shape)   # (10000, 28, 28)\n",
    "print(\"y_test.shape =\", y_test.shape)   # (10000,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d68f381",
   "metadata": {},
   "source": [
    "미니배치를 위해서 주어진 데이터에서 무작위로 N개 추출하는 방법은 `np.random.choice()`를 사용한다.  \n",
    "다음은 $N=10$일 때의 경우이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3de5f7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "x_batch = x_train[batch_mask]\n",
    "y_batch = y_train[batch_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd8a97b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_batch.shape = (10, 28, 28)\n",
      "y_batch.shape = (10,)\n"
     ]
    }
   ],
   "source": [
    "print(\"x_batch.shape =\", x_batch.shape)  # (10, 28, 28)\n",
    "print(\"y_batch.shape =\", y_batch.shape)  # (10,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9d7fe4",
   "metadata": {},
   "source": [
    "##### 교차 엔트로피 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b79fed00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y_pred, y_true):\n",
    "    if y_pred.ndim == 1:\n",
    "        y_pred = y_pred.reshape(1, y_pred.size)\n",
    "        y_true = y_true.reshape(1, y_true.size)\n",
    "\n",
    "    batch_size = y_pred.shape[0]\n",
    "    delta = 1e-7\n",
    "    #return -np.sum(y_true * np.log(y_pred + delta)) / batch_size\n",
    "    return -np.sum(np.log(y_pred[np.arange(batch_size), y_true] + delta)) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a0f1e7",
   "metadata": {},
   "source": [
    "## 경사 하강법\n",
    "이렇게 구현한 교차 엔트로피 오차, 즉, 손실 함수를 최소화함으로써 모델의 가중치 매개변수를 찾을 수 있다.  \n",
    "손실 함수의 최솟값은 미분을 통해, 함수의 기울기만큼 가중치 매개변수를 변화시켜가는 과정을 반복하여 찾는다.  \n",
    "이러한 방법을 **경사 하강법** 이라고 한다.  \n",
    "수식으론 다음과 같다.  \n",
    "$$\n",
    "\\begin{align}\n",
    "x_0 &\\triangleq x_0 - \\alpha \\frac{\\partial f}{\\partial x_0} \\\\\n",
    "x_1 &\\triangleq x_1 - \\alpha \\frac{\\partial f}{\\partial x_1} \\\\\n",
    "&~~~\\vdots\n",
    "\\end{align}\n",
    "$$\n",
    "이때 $\\alpha$를 학습률(Learning rate)라고 한다. 값에 따라 각 매개변수의 값을 얼마나 변화시키는지를 정한다.  \n",
    "적절한 학습률 값을 정하는 것은 중요한데, 너무 작으면 손실 함수가 최솟값에 가깝게 수렴하는데 속도가 너무 오래 걸리고, 너무 크면 손실 함수가 최솟값에 가깝게 수렴하지 못하고 그 주위에서 '진동' 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b1d59d",
   "metadata": {},
   "source": [
    "#### CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "373e6406",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4  # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "\n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "\n",
    "        # f(x + h)\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "\n",
    "        # f(x - h)\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "\n",
    "        grad[idx] = (fxh1 - fxh2) / (2 * h)\n",
    "        x[idx] = tmp_val  # restore value\n",
    "\n",
    "    return grad\n",
    "\n",
    "\n",
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x.copy()\n",
    "\n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a07966c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result = [-6.11110793e-10  8.14814391e-10]\n"
     ]
    }
   ],
   "source": [
    "def fuction_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "result = gradient_descent(fuction_2, init_x=init_x, lr=0.1, step_num=100)\n",
    "print(\"result =\", result)  # [ -6.11110793e-10   8.14814391e-10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c7df3608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result = [-2.58983747e+13 -1.29524862e+12]\n"
     ]
    }
   ],
   "source": [
    "# 학습률이 큰 경우\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "result = gradient_descent(fuction_2, init_x=init_x, lr=10.0, step_num=100)\n",
    "print(\"result =\", result)  # [ -2.58983747e+13   3.45244996e+13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e7077cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result = [-2.99999994  3.99999992]\n"
     ]
    }
   ],
   "source": [
    "# 학습률이 작은 경우\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "result = gradient_descent(fuction_2, init_x=init_x, lr=1e-10, step_num=100)\n",
    "print(\"result =\", result)  # [ -2.99999999   3.99999999]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4f5274",
   "metadata": {},
   "source": [
    "### 신경망에서의 기울기\n",
    "이를 신경망 학습에 적용하자. 신경망에서 가중치 $\\bold{W}$, 손실 함수 $L$에 대해서, 기울기는 다음과 같이 나타낼 수 있다.  \n",
    "$$\n",
    "\\begin{align}\n",
    "\\bold{W} &= \\begin{pmatrix}\n",
    "w_{11} & w_{12} & w_{13} \\\\\n",
    "w_{21} & w_{22} & w_{23}\n",
    "\\end{pmatrix} \\\\\\\\\n",
    "\\frac{\\partial L}{\\partial \\bold{W}} &= \\begin{pmatrix}\n",
    "\\frac{\\partial L}{\\partial w_{11}} & \\frac{\\partial L}{\\partial w_{12}} & \\frac{\\partial L}{\\partial w_{13}} \\\\\n",
    "\\frac{\\partial L}{\\partial w_{21}} & \\frac{\\partial L}{\\partial w_{22}} & \\frac{\\partial L}{\\partial w_{23}}\n",
    "\\end{pmatrix}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc01fc0",
   "metadata": {},
   "source": [
    "#### CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5fab1480",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2, 3)  # weight initialization\n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d643b1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weight =\n",
      " [[-0.32688493  0.66694564 -0.33586083]\n",
      " [ 0.84330568 -0.52344912 -0.53677751]]\n"
     ]
    }
   ],
   "source": [
    "net = simpleNet()\n",
    "print(\"Initial weight =\\n\", net.W)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1a7c1f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class probabilities = [ 0.56284415 -0.07093683 -0.68461625]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "print(\"Predicted class probabilities =\", p)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "acd91013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class = 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted class =\", np.argmax(p))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fe36d514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss value = 2.4266863149107696\n"
     ]
    }
   ],
   "source": [
    "t = np.array([0, 0, 1])  # true label\n",
    "loss = net.loss(x, t)\n",
    "print(\"Loss value =\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "16e8fcc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of weights =\n",
      " [[-0.20980036 -0.07461792  0.28441828]\n",
      " [-0.31470054 -0.11192688  0.42662741]]\n"
     ]
    }
   ],
   "source": [
    "def f(W):\n",
    "    return net.loss(x, t)\n",
    "\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4  # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x)  # f(x+h)\n",
    "\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)  # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "\n",
    "        x[idx] = tmp_val  # 값 복원\n",
    "        it.iternext()\n",
    "\n",
    "    return grad\n",
    "\n",
    "dW = numerical_gradient(f, net.W)\n",
    "print(\"Gradient of weights =\\n\", dW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7e2ccc",
   "metadata": {},
   "source": [
    "## 학습 알고리즘 구현\n",
    "지금까지, 신경망 학습에 대해서 살표보았다. 절차는 다음과 같다.  \n",
    "1. 미니배치 :  \n",
    "    훈련 데이터 중 일부를 무작이로 추출한다.\n",
    "2. 기울기 산출 :  \n",
    "    미니배치 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다.\n",
    "3. 매개변수 갱신 :   \n",
    "    가중치 매개변수를 기울기 값에 따라 갱신한다.\n",
    "4. 위의 과정을 반복한다.  \n",
    "   \n",
    "미니배치는 무작위로 선정되기 때문에 이를 **확률적 경사 하강법**이라 한다.  \n",
    "이제 손글씨 숫자를 학습하는 신경망을 구현하도록 한다.  \n",
    "그를 위해 은닉층이 1개인 신경망으로 구성한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23826ab4",
   "metadata": {},
   "source": [
    "### 신경망 클래스 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a6f43f",
   "metadata": {},
   "source": [
    "#### CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fd6959bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # Weight initialization\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = np.maximum(0, a1)  # ReLU activation\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    # Compute loss\n",
    "    # x: input data, t: true labels\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    # Compute numerical gradients\n",
    "    # x: input data, t: true labels\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ed0f46c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "net = LayerNet(input_size=784, hidden_size=100, output_size=10)\n",
    "print(net.params['W1'].shape)  # (784, 100)\n",
    "print(net.params['b1'].shape)  # (100,)\n",
    "print(net.params['W2'].shape)  # (100, 10)\n",
    "print(net.params['b2'].shape)  # (10,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1733a1c3",
   "metadata": {},
   "source": [
    "### 미니배치 학습 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30146df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 6.906243988997322\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[80]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     24\u001b[39m y_batch = y_train[batch_mask]\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Gradient calculation\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m grads = \u001b[43mnetwork\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumerical_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Parameter update\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m'\u001b[39m\u001b[33mW1\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mb1\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mW2\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mb2\u001b[39m\u001b[33m'\u001b[39m):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[76]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36mLayerNet.numerical_gradient\u001b[39m\u001b[34m(self, x, t)\u001b[39m\n\u001b[32m     38\u001b[39m loss_W = \u001b[38;5;28;01mlambda\u001b[39;00m W: \u001b[38;5;28mself\u001b[39m.loss(x, t)\n\u001b[32m     40\u001b[39m grads = {}\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m grads[\u001b[33m'\u001b[39m\u001b[33mW1\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mnumerical_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_W\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mW1\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m grads[\u001b[33m'\u001b[39m\u001b[33mb1\u001b[39m\u001b[33m'\u001b[39m] = numerical_gradient(loss_W, \u001b[38;5;28mself\u001b[39m.params[\u001b[33m'\u001b[39m\u001b[33mb1\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     43\u001b[39m grads[\u001b[33m'\u001b[39m\u001b[33mW2\u001b[39m\u001b[33m'\u001b[39m] = numerical_gradient(loss_W, \u001b[38;5;28mself\u001b[39m.params[\u001b[33m'\u001b[39m\u001b[33mW2\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[75]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mnumerical_gradient\u001b[39m\u001b[34m(f, x)\u001b[39m\n\u001b[32m     11\u001b[39m tmp_val = x[idx]\n\u001b[32m     12\u001b[39m x[idx] = \u001b[38;5;28mfloat\u001b[39m(tmp_val) + h\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m fxh1 = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# f(x+h)\u001b[39;00m\n\u001b[32m     15\u001b[39m x[idx] = tmp_val - h\n\u001b[32m     16\u001b[39m fxh2 = f(x)  \u001b[38;5;66;03m# f(x-h)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[76]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mLayerNet.numerical_gradient.<locals>.<lambda>\u001b[39m\u001b[34m(W)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnumerical_gradient\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     loss_W = \u001b[38;5;28;01mlambda\u001b[39;00m W: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m     grads = {}\n\u001b[32m     41\u001b[39m     grads[\u001b[33m'\u001b[39m\u001b[33mW1\u001b[39m\u001b[33m'\u001b[39m] = numerical_gradient(loss_W, \u001b[38;5;28mself\u001b[39m.params[\u001b[33m'\u001b[39m\u001b[33mW1\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[76]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mLayerNet.loss\u001b[39m\u001b[34m(self, x, t)\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mloss\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     y = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cross_entropy_error(y, t)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[76]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mLayerNet.predict\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     12\u001b[39m b1, b2 = \u001b[38;5;28mself\u001b[39m.params[\u001b[33m'\u001b[39m\u001b[33mb1\u001b[39m\u001b[33m'\u001b[39m], \u001b[38;5;28mself\u001b[39m.params[\u001b[33m'\u001b[39m\u001b[33mb2\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     14\u001b[39m a1 = np.dot(x, W1) + b1\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m z1 = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmaximum\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma1\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# ReLU activation\u001b[39;00m\n\u001b[32m     16\u001b[39m a2 = np.dot(z1, W2) + b2\n\u001b[32m     17\u001b[39m y = softmax(a2)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "network = LayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], 784)\n",
    "x_test = x_test.reshape(x_test.shape[0], 784)\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "iters_num = 1000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # Mini-batch selection\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    y_batch = y_train[batch_mask]\n",
    "    \n",
    "    # Gradient calculation\n",
    "    grads = network.numerical_gradient(x_batch, y_batch)\n",
    "    \n",
    "    # Parameter update\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grads[key]\n",
    "    \n",
    "    # Record loss\n",
    "    loss = network.loss(x_batch, y_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, y_train)\n",
    "        test_acc = network.accuracy(x_test, y_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(f\"Epoch {i // iter_per_epoch}: Train Accuracy = {train_acc}, Test Accuracy = {test_acc}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ff60c1",
   "metadata": {},
   "source": [
    "여기서 수치 미분법을 사용하였는데, 이는 속도가 매우 느리므로, 코드 실행은 생략한다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
